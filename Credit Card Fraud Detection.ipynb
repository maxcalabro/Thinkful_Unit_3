{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Credit Card Fraud With Supervised Learning\n",
    "\n",
    "The data set we'll be using is available on Kaggle. The features have already been run through PCA, so no decomposition is necessary. Let's take a look at what we have and see how we can model it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('creditcard.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 284807)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_data['Class']==1).sum(), len(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest hurdle here is likely to be the class imbalance. Fewer than 500 of the 284,000 records are fraudulent. We can try a few things to make this work. Let's start by resampling the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284807, 30), (284807,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = raw_data.loc[:, ~raw_data.columns.isin(['Class'])]\n",
    "Y_full = raw_data['Class']\n",
    "X_full.shape, Y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((492, 31), (284315, 31))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos = raw_data[raw_data['Class']==1]\n",
    "data_neg = raw_data[raw_data['Class']==0]\n",
    "\n",
    "data_pos.shape, data_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29687</th>\n",
       "      <td>35585.0</td>\n",
       "      <td>-2.019001</td>\n",
       "      <td>1.491270</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.817253</td>\n",
       "      <td>0.973252</td>\n",
       "      <td>-0.639268</td>\n",
       "      <td>-0.974073</td>\n",
       "      <td>-3.146929</td>\n",
       "      <td>-0.003159</td>\n",
       "      <td>...</td>\n",
       "      <td>2.839596</td>\n",
       "      <td>-1.185443</td>\n",
       "      <td>-0.142812</td>\n",
       "      <td>-0.086103</td>\n",
       "      <td>-0.329113</td>\n",
       "      <td>0.523601</td>\n",
       "      <td>0.626283</td>\n",
       "      <td>0.152440</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64329</th>\n",
       "      <td>51112.0</td>\n",
       "      <td>-9.848776</td>\n",
       "      <td>7.365546</td>\n",
       "      <td>-12.898538</td>\n",
       "      <td>4.273323</td>\n",
       "      <td>-7.611991</td>\n",
       "      <td>-3.427045</td>\n",
       "      <td>-8.350808</td>\n",
       "      <td>6.863604</td>\n",
       "      <td>-2.387567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931958</td>\n",
       "      <td>-0.874467</td>\n",
       "      <td>-0.192639</td>\n",
       "      <td>-0.035426</td>\n",
       "      <td>0.538665</td>\n",
       "      <td>-0.263934</td>\n",
       "      <td>1.134095</td>\n",
       "      <td>0.225973</td>\n",
       "      <td>99.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42887</th>\n",
       "      <td>41285.0</td>\n",
       "      <td>-12.835760</td>\n",
       "      <td>6.574615</td>\n",
       "      <td>-12.788462</td>\n",
       "      <td>8.786257</td>\n",
       "      <td>-10.723121</td>\n",
       "      <td>-2.813536</td>\n",
       "      <td>-14.248847</td>\n",
       "      <td>7.960521</td>\n",
       "      <td>-7.718751</td>\n",
       "      <td>...</td>\n",
       "      <td>2.679490</td>\n",
       "      <td>-0.047335</td>\n",
       "      <td>-0.836982</td>\n",
       "      <td>0.625349</td>\n",
       "      <td>0.125865</td>\n",
       "      <td>0.177624</td>\n",
       "      <td>-0.817680</td>\n",
       "      <td>-0.521030</td>\n",
       "      <td>37.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157591</th>\n",
       "      <td>110099.0</td>\n",
       "      <td>2.100253</td>\n",
       "      <td>0.198889</td>\n",
       "      <td>-1.794350</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>0.448126</td>\n",
       "      <td>-0.954524</td>\n",
       "      <td>-0.011194</td>\n",
       "      <td>-0.300150</td>\n",
       "      <td>1.840955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020492</td>\n",
       "      <td>0.244313</td>\n",
       "      <td>0.062023</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.055294</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>-0.102058</td>\n",
       "      <td>-0.045644</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249828</th>\n",
       "      <td>154599.0</td>\n",
       "      <td>0.667714</td>\n",
       "      <td>3.041502</td>\n",
       "      <td>-5.845112</td>\n",
       "      <td>5.967587</td>\n",
       "      <td>0.213863</td>\n",
       "      <td>-1.462923</td>\n",
       "      <td>-2.688761</td>\n",
       "      <td>0.677764</td>\n",
       "      <td>-3.447596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329760</td>\n",
       "      <td>-0.941383</td>\n",
       "      <td>-0.006075</td>\n",
       "      <td>-0.958925</td>\n",
       "      <td>0.239298</td>\n",
       "      <td>-0.067356</td>\n",
       "      <td>0.821048</td>\n",
       "      <td>0.426175</td>\n",
       "      <td>6.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1        V2         V3        V4         V5  \\\n",
       "29687    35585.0  -2.019001  1.491270   0.005222  0.817253   0.973252   \n",
       "64329    51112.0  -9.848776  7.365546 -12.898538  4.273323  -7.611991   \n",
       "42887    41285.0 -12.835760  6.574615 -12.788462  8.786257 -10.723121   \n",
       "157591  110099.0   2.100253  0.198889  -1.794350  0.561776   0.448126   \n",
       "249828  154599.0   0.667714  3.041502  -5.845112  5.967587   0.213863   \n",
       "\n",
       "              V6         V7        V8        V9  ...         V21       V22  \\\n",
       "29687  -0.639268  -0.974073 -3.146929 -0.003159  ...    2.839596 -1.185443   \n",
       "64329  -3.427045  -8.350808  6.863604 -2.387567  ...    0.931958 -0.874467   \n",
       "42887  -2.813536 -14.248847  7.960521 -7.718751  ...    2.679490 -0.047335   \n",
       "157591 -0.954524  -0.011194 -0.300150  1.840955  ...   -0.020492  0.244313   \n",
       "249828 -1.462923  -2.688761  0.677764 -3.447596  ...    0.329760 -0.941383   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "29687  -0.142812 -0.086103 -0.329113  0.523601  0.626283  0.152440    0.76   \n",
       "64329  -0.192639 -0.035426  0.538665 -0.263934  1.134095  0.225973   99.99   \n",
       "42887  -0.836982  0.625349  0.125865  0.177624 -0.817680 -0.521030   37.32   \n",
       "157591  0.062023  0.453100  0.055294  0.622047 -0.102058 -0.045644    1.50   \n",
       "249828 -0.006075 -0.958925  0.239298 -0.067356  0.821048  0.426175    6.74   \n",
       "\n",
       "        Class  \n",
       "29687       1  \n",
       "64329       1  \n",
       "42887       1  \n",
       "157591      0  \n",
       "249828      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos_resamp = resample(data_pos, n_samples=20000)\n",
    "data_neg_resamp = resample(data_neg, n_samples=20000)\n",
    "\n",
    "data_resamp = pd.concat([data_pos_resamp, data_neg_resamp])\n",
    "data_resamp = data_resamp.sample(frac=1)\n",
    "\n",
    "data_resamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try running some models on it. The features of this data set are the result of a PCA, so they should already be orthagonal to each other. We may find it useful to run some feature selection, but for now let's use them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 30), (40000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_resamp.loc[:, ~data_resamp.columns.isin(['Class'])]\n",
    "Y = data_resamp['Class']\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.909   ,  0.897375,  0.910125,  0.906125,  0.90575 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "\n",
    "cross_val_score(bnb, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.981375,  0.979375,  0.987125,  0.97975 ,  0.972625])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(\n",
    "        max_depth=10,\n",
    "        max_features=4\n",
    ")\n",
    "\n",
    "cross_val_score(dtc, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97025 ,  0.965   ,  0.971125,  0.97    ,  0.967   ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        max_depth=6\n",
    ")\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree runs a lot faster than Random Forest, and is doing just as well at the moment. Let's fit it and see what it does on the actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[280479,   3836],\n",
       "       [     4,    488]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(X, Y)\n",
    "\n",
    "Y_pred = dtc.predict(X_full)\n",
    "\n",
    "confusion_matrix(Y_full, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, this is actually pretty great! (Also, I realize confusion matrix isn't the best way to asses this result). But let's just take a really quick look at it. In fraud detection, we are WAY more concerned about missing fraud than we are about mis-classifying non-fraud. And even with this incredibly simple model, we only missed 4 of the 492 examples of fraud. We also sent 5,000 emails alerting people of potential fraud that didn't happen, but hey, they can just confirm that it wasn't fraud, right? One problem here is that we fit on the entire set of positive examples, so we're prone to overfitting. It's a shame we have so few examples, but we really do have to pull some of those out to act as a test set.\n",
    "\n",
    "Let's try this again, but holding out some of the original data to act as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 30), (100000,), (234465, 30), (234465,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate out n_test positive examples for the test set. 150 (30%) seems to give the most stable results.\n",
    "n_test = 150\n",
    "n_train = 50000\n",
    "\n",
    "data_pos = data_pos.sample(frac=1)\n",
    "data_pos_train = data_pos[n_test:]\n",
    "data_pos_test = data_pos[:n_test]\n",
    "\n",
    "# Separate out n_train negative examples for training set\n",
    "data_neg = data_neg.sample(frac=1)\n",
    "data_neg_train = data_neg[:n_train]\n",
    "data_neg_test = data_neg[n_train:]\n",
    "\n",
    "# Upsample the positive training examples so we're back to our 1:1 ratio\n",
    "data_pos_train_resamp = resample(data_pos_train, n_samples=n_train)\n",
    "\n",
    "# Recombine to make training and test datasets. Test set now only has 100 positive examples.\n",
    "data_train = pd.concat([data_pos_train_resamp, data_neg_train]).sample(frac=1)\n",
    "data_test = pd.concat([data_pos_test, data_neg_test]).sample(frac=1)\n",
    "\n",
    "X_train = data_train.loc[:, ~data_train.columns.isin(['Class'])]\n",
    "Y_train = data_train['Class']\n",
    "X_test = data_test.loc[:, ~data_test.columns.isin(['Class'])]\n",
    "Y_test = data_test['Class']\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99235,  0.98305,  0.98655,  0.98455,  0.9757 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(\n",
    "        max_depth=10,\n",
    "        max_features=4\n",
    ")\n",
    "\n",
    "cross_val_score(dtc, X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[232795,   1520],\n",
       "       [    33,    117]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(X_train,Y_train)\n",
    "Y_pred = dtc.predict(X_test)\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we didn't do quite as well here, but we're still doing something at least! Classified over 80% of the positive examples correctly, while getting better than 98% accuracy on negative examples. Let's take a look at the area under the precision-recall curve (AUPRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42516672126919758"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(Y_test, Y_pred)\n",
    "auc(precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can do better. First, how does our original result compare to this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auprc(Y, Y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(Y, Y_pred)\n",
    "    return auc(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55804757064528576"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc(Y_full, dtc.predict(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is with 99% accuracy on positive examples, so doing better will mean fewer false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.731748973522\n",
      "[[234271     44]\n",
      " [    38    112]]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "        n_estimators=30,\n",
    "        max_depth=20\n",
    ")\n",
    "\n",
    "rfc.fit(X_train, Y_train)\n",
    "Y_pred = rfc.predict(X_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this is amazing! Using Random Forest, we still aren't getting some of those positives in the test set, but we've reduced false positives to barely any. Let's see how an SVC does. We actually get fewer false negatives with n_train of a couple thousand, but our lowest AUPRC is with n_train about 50,000. Let's see how other models compare to this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svc = SVC(C=1)\n",
    "#\n",
    "#svc.fit(X_train, Y_train)\n",
    "#Y_pred = svc.predict(X_test)\n",
    "#print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "#print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh, that takes so much longer to run than random forest and didn't do anything good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.417386416985\n",
      "[[232927   1388]\n",
      " [    36    114]]\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train,Y_train)\n",
    "Y_pred = bnb.predict(X_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.449141282402\n",
      "[[229436   4879]\n",
      " [    19    131]]\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "\n",
    "lgr.fit(X_train,Y_train)\n",
    "Y_pred = lgr.predict(X_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.467966366829\n",
      "[[233977    338]\n",
      " [    45    105]]\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=20, max_features=4)\n",
    "\n",
    "dtc.fit(X_train,Y_train)\n",
    "Y_pred = dtc.predict(X_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.116774317811\n",
      "[[232045   2270]\n",
      " [   117     33]]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's do some feature selection and see if we can keep the quality of our results with fewer features. Note that PCA has already been run on our starting feature set. We'll do RFE, SelectKBest, and Random Forest importances, and run them on our best model, RFC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.712036006322\n",
      "[[234264     51]\n",
      " [    39    111]]\n"
     ]
    }
   ],
   "source": [
    "rfe = RFE(dtc, n_features_to_select=8)\n",
    "\n",
    "rfe.fit(X_train, Y_train)\n",
    "X_rfe_train = rfe.transform(X_train)\n",
    "X_rfe_test = rfe.transform(X_test)\n",
    "\n",
    "rfc.fit(X_rfe_train, Y_train)\n",
    "Y_pred = rfc.predict(X_rfe_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the best 8 features with RFE seems to maintain our result -- pretty good reduction from 29!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.72913026772\n",
      "[[234266     49]\n",
      " [    36    114]]\n"
     ]
    }
   ],
   "source": [
    "skb = SelectKBest(k=8)\n",
    "\n",
    "skb.fit(X_train, Y_train)\n",
    "X_skb_train = skb.transform(X_train)\n",
    "X_skb_test = skb.transform(X_test)\n",
    "\n",
    "rfc.fit(X_skb_train, Y_train)\n",
    "Y_pred = rfc.predict(X_skb_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 features from SelectKBest does the job as well. We have to re-run our model to get our original feature_importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_feat = RandomForestClassifier(\n",
    "        n_estimators=30,\n",
    "        max_depth=20\n",
    ")\n",
    "\n",
    "rfc_feat.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.716715122695\n",
      "[[234260     55]\n",
      " [    36    114]]\n"
     ]
    }
   ],
   "source": [
    "n_feat = 7\n",
    "X_rfc_train = X_train.loc[:, X_train.columns[np.argsort(rfc_feat.feature_importances_)[-n_feat:]]]\n",
    "X_rfc_test = X_test.loc[:, X_test.columns[np.argsort(rfc_feat.feature_importances_)[-n_feat:]]]\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "        n_estimators=30,\n",
    "        max_depth=20\n",
    ")\n",
    "\n",
    "rfc.fit(X_rfc_train, Y_train)\n",
    "Y_pred = rfc.predict(X_rfc_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results here -- need about 7-8 features to get the full accuracy. Finally, let's try gradient boosting and see how that does. We'll just use the feature_importance result for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC: 0.663019105738\n",
      "[[234228     87]\n",
      " [    36    114]]\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=.3,\n",
    "            max_depth=4\n",
    ")\n",
    "\n",
    "gbc.fit(X_rfc_train, Y_train)\n",
    "Y_pred = gbc.predict(X_rfc_test)\n",
    "print('AUPRC: ' + str(auprc(Y_test, Y_pred)))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is starting to get up into the neighborhood of our Random Forest classifier, but it is already slower. We could tune it with a grid search, but I'm pretty happy with RFC for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Although we only have 500 examples of fraud in our data set, we're able to classify quite well by upsampling those positive examples. We can use feature selection to reduce our initial (already decomposed) feature set from 29 to 8 without losing much of the variance in our model. Because 29 still isn't that many features, I would recommend keeping all of those features for the minor improvement in our result.\n",
    "\n",
    "Because of the extreme class imbalance, accuracy wasn't a suitable measure for our models, so we used the area under the precision-recall curve. Of our models, random forest performed best, with an AUPRC score of 0.73. Gradient boosting did fairly well (0.66), but most of the other models did not perform well ( < 0.50 ). They all continued to find a large number ( > 1000 ) of false positives. \n",
    "\n",
    "Our random forest would likely do even better if we had more positive examples of fraud. There are surely more than 500 ways a transaction can be fraudulent, and unfortunately random forest is weak at extrapolating to examples it hasn't seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
